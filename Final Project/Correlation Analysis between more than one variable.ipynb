{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f7bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d14347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from NASA exoplenet archive \n",
    "df = pd.read_csv(r\"C:\\Users\\julia\\Downloads\\csv_main.csv\", skiprows=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206df1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_linear_regression_analysis(df, predictor_vars, response_var, x_labels, y_label, log=False):\n",
    "    \"\"\"\n",
    "    Performs multiple linear regression analysis between multiple predictor variables and a response variable \n",
    "    in a DataFrame with custom axis labels.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    predictor_vars: List of column names to be used as predictor variables.\n",
    "    response_var: The name of the column to be used as the response variable.\n",
    "    x_labels: Labels for the x-axes (for each predictor variable).\n",
    "    y_label: Label for the y-axis (response variable).\n",
    "    log : Whether to use logarithmic scale for the plot.\n",
    "    \"\"\"\n",
    "    if len(predictor_vars) != len(x_labels):\n",
    "        raise ValueError(\"The number of predictor variables and x-labels must be the same.\")\n",
    "\n",
    "    # Filtering out rows with NaN values in the specified columns\n",
    "    df_filtered = df.dropna(subset=predictor_vars + [response_var])\n",
    "    X = df_filtered[predictor_vars]\n",
    "    y = df_filtered[response_var]  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Creating a multiple linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Displaying the results\n",
    "    print(\"Multiple Linear Regression Analysis\")\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"R-squared:\", r2)\n",
    "  \n",
    "    \n",
    "def polynomial_regression_analysis(df, predictor_vars, response_var, degree, x_labels, y_label, log=False):\n",
    "    if len(predictor_vars) != len(x_labels):\n",
    "        raise ValueError(\"Number of predictors and x-labels must be the same.\")\n",
    "\n",
    "    # Filtering out rows with NaN values\n",
    "    df_filtered = df.dropna(subset=predictor_vars + [response_var])\n",
    "    X = df_filtered[predictor_vars] \n",
    "    y = df_filtered[response_var] \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_poly = poly.fit_transform(X_train)\n",
    "    model_poly = LinearRegression()\n",
    "    model_poly.fit(X_poly, y_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    y_pred_poly = model_poly.predict(X_test_poly)\n",
    "    mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "    r2_poly = r2_score(y_test, y_pred_poly)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Polynomial Regression (Degree = {degree}):\")\n",
    "    print(\"Mean Squared Error:\", mse_poly)\n",
    "    print(\"R-squared:\", r2_poly)\n",
    "             \n",
    "        \n",
    "def random_forest_regression_analysis(df, predictor_vars, response_var, x_labels, y_label, n_estimators=100, show_feature_importance=False, log=False):\n",
    "    if len(predictor_vars) != len(x_labels):\n",
    "        raise ValueError(\"Number of predictors and x-labels must be the same.\")\n",
    "\n",
    "    # Filtering out rows with NaN values\n",
    "    df_filtered = df.dropna(subset=predictor_vars + [response_var])\n",
    "    X = df_filtered[predictor_vars]  \n",
    "    y = df_filtered[response_var]  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "    # Displaying the results\n",
    "    print(\"Random Forest Regression:\")\n",
    "    print(\"Mean Squared Error:\", mse_rf)\n",
    "    print(\"R-squared:\", r2_rf)\n",
    "\n",
    "    # Feature importance can be shown\n",
    "    if show_feature_importance:\n",
    "        feature_importances = rf_model.feature_importances_\n",
    "        plt.bar(x_labels, feature_importances)\n",
    "        plt.title('Feature Importances')\n",
    "        plt.show()\n",
    "        \n",
    "def gradient_boosting_regression_analysis(df, predictor_vars, response_var, x_labels, y_label, n_estimators=100, learning_rate=0.1, log=False):\n",
    "    if len(predictor_vars) != len(x_labels):\n",
    "        raise ValueError(\"Number of predictors and x-labels must be the same.\")\n",
    "\n",
    "    # Filtering out rows with NaN values\n",
    "    df_filtered = df.dropna(subset=predictor_vars + [response_var])\n",
    "    X = df_filtered[predictor_vars]  \n",
    "    y = df_filtered[response_var]  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Creating a gradient boosting model\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_model.predict(X_test)\n",
    "    mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "    r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "    # Displaying the results\n",
    "    print(\"Gradient Boosting Regression:\")\n",
    "    print(\"Mean Squared Error:\", mse_gb)\n",
    "    print(\"R-squared:\", r2_gb)\n",
    "    \n",
    "def neural_network_regression(df, predictor_vars, response_var, x_labels, y_label, log=False):\n",
    "    if len(predictor_vars) != len(x_labels):\n",
    "        raise ValueError(\"Number of predictors and x-labels must be the same.\")\n",
    "\n",
    "    # Filtering out rows with NaN values\n",
    "    df_filtered = df.dropna(subset=predictor_vars + [response_var])\n",
    "    X = df_filtered[predictor_vars]\n",
    "    y = df_filtered[response_var]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Creating a neural network model\n",
    "    nn_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=10000, random_state=0)\n",
    "    nn_model.fit(X_train, y_train)\n",
    "    y_pred_nn = nn_model.predict(X_test)\n",
    "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "    r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "    # Displaying the results\n",
    "    print(\"Neural Network Regression:\")\n",
    "    print(\"Mean Squared Error:\", mse_nn)\n",
    "    print(\"R-squared:\", r2_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb78f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Linear Regression Analysis\n",
      "Mean Squared Error: 1761567.5182650192\n",
      "R-squared: -1.2780374619588435\n",
      "Polynomial Regression (Degree = 2):\n",
      "Mean Squared Error: 93728155.88291742\n",
      "R-squared: -120.20809910930791\n",
      "Random Forest Regression:\n",
      "Mean Squared Error: 712905.2909538792\n",
      "R-squared: 0.07807964055722372\n",
      "Neural Network Regression:\n",
      "Mean Squared Error: 785858.5270230457\n",
      "R-squared: -0.016262587608047108\n",
      "Gradient Boosting Regression:\n",
      "Mean Squared Error: 697842.8492984007\n",
      "R-squared: 0.09755820496305723\n"
     ]
    }
   ],
   "source": [
    "# Predictor Variables: Radius, Semi-Major Axis\n",
    "# Response Variable: Mass\n",
    "\n",
    "multiple_linear_regression_analysis(df, ['pl_rade', 'pl_orbsmax'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis'], 'Planet Mass')\n",
    "polynomial_regression_analysis(df, ['pl_rade', 'pl_orbsmax'], 'pl_bmasse', 2, ['Planet Radius', 'Semi-Major Axis'], 'Planet Mass')\n",
    "random_forest_regression_analysis(df, ['pl_rade', 'pl_orbsmax'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis'], 'Planet Mass', n_estimators=100, show_feature_importance=False, log=False)  \n",
    "neural_network_regression(df, ['pl_rade', 'pl_orbsmax'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis'], 'Planet Mass', log=False)\n",
    "gradient_boosting_regression_analysis(df, ['pl_rade', 'pl_orbsmax'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis'], 'Planet Mass', n_estimators=100, learning_rate=0.1, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bd019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Linear Regression Analysis\n",
      "Mean Squared Error: 0.018362028864829374\n",
      "R-squared: 0.1403462386933113\n",
      "Polynomial Regression (Degree = 2):\n",
      "Mean Squared Error: 0.018517262826907042\n",
      "R-squared: 0.1330786616534878\n",
      "Random Forest Regression:\n",
      "Mean Squared Error: 0.018118664151192124\n",
      "R-squared: 0.1517398266779324\n",
      "Neural Network Regression:\n",
      "Mean Squared Error: 0.4162216476799651\n",
      "R-squared: -18.486218412971343\n",
      "Gradient Boosting Regression:\n",
      "Mean Squared Error: 0.018591379710852386\n",
      "R-squared: 0.12960873692300323\n"
     ]
    }
   ],
   "source": [
    "# Predictor Variables: Radius, Mass, Temperature, \n",
    "# Response Variable: Eccentricity\n",
    "\n",
    "multiple_linear_regression_analysis(df, ['pl_rade', 'pl_bmasse', 'pl_eqt'], 'pl_orbeccen', ['Planet Radius', 'Planet Mass', 'Planet Temperature'], 'Eccentricity')\n",
    "polynomial_regression_analysis(df, ['pl_rade', 'pl_bmasse', 'pl_eqt'], 'pl_orbeccen', 2, ['Planet Radius', 'Planet Mass', 'Planet Temperature'], 'Eccentricity')\n",
    "random_forest_regression_analysis(df, ['pl_rade', 'pl_bmasse', 'pl_eqt'], 'pl_orbeccen', ['Planet Radius', 'Planet Mass', 'Planet Temperature'], 'Eccentricity', n_estimators=100, show_feature_importance=False, log=False)  \n",
    "neural_network_regression(df, ['pl_rade', 'pl_bmasse', 'pl_eqt'], 'pl_orbeccen', ['Planet Radius', 'Planet Mass', 'Planet Temperature'], 'Eccentricity', log=False)\n",
    "gradient_boosting_regression_analysis(df, ['pl_rade', 'pl_bmasse', 'pl_eqt'], 'pl_orbeccen', ['Planet Radius', 'Planet Mass', 'Planet Temperature'], 'Eccentricity', n_estimators=100, learning_rate=0.1, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33be915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Linear Regression Analysis\n",
      "Mean Squared Error: 124797.79400127879\n",
      "R-squared: 0.5354804103411963\n",
      "Polynomial Regression (Degree = 2):\n",
      "Mean Squared Error: 0.018517262826907042\n",
      "R-squared: 0.1330786616534878\n",
      "Random Forest Regression:\n",
      "Mean Squared Error: 17273.117504977377\n",
      "R-squared: 0.935706383916865\n",
      "Neural Network Regression:\n",
      "Mean Squared Error: 451726.2074242133\n",
      "R-squared: -0.6814053019933435\n",
      "Gradient Boosting Regression:\n",
      "Mean Squared Error: 14605.996448430697\n",
      "R-squared: 0.9456338829457719\n"
     ]
    }
   ],
   "source": [
    "# Predictor Variables: Planet Radius, Semi-Major Axis, Planet Mass,Stellar Metallicity, Stellar Mass\n",
    "# Response Variable: Planet Temperature\n",
    "'''\n",
    "# High Correlation\n",
    "'''\n",
    "\n",
    "multiple_linear_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_bmasse', 'st_met', 'st_mass'], 'pl_eqt', ['Planet Radius', 'Semi-Major Axis', 'Planet Mass', 'Stellar Metallicity', 'Stellar Mass'], 'Planet Temperature')\n",
    "polynomial_regression_analysis(df, ['pl_rade', 'pl_bmasse', 'pl_eqt'], 'pl_orbeccen', 2, ['Planet Radius', 'Planet Mass', 'Planet Temperature'], 'Eccentricity')\n",
    "random_forest_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_bmasse', 'st_met', 'st_mass'], 'pl_eqt', ['Planet Radius', 'Semi-Major Axis', 'Planet Mass', 'Stellar Metallicity', 'Stellar Mass'], 'Planet Temperature', n_estimators=100, show_feature_importance=False, log=False)  \n",
    "neural_network_regression(df, ['pl_rade', 'pl_orbsmax', 'pl_bmasse', 'st_met', 'st_mass'], 'pl_eqt', ['Planet Radius', 'Semi-Major Axis', 'Planet Mass', 'Stellar Metallicity', 'Stellar Mass'], 'Planet Temperature', log=False)\n",
    "gradient_boosting_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_bmasse', 'st_met', 'st_mass'], 'pl_eqt', ['Planet Radius', 'Semi-Major Axis', 'Planet Mass', 'Stellar Metallicity', 'Stellar Mass'], 'Planet Temperature', n_estimators=100, learning_rate=0.1, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c75f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Linear Regression Analysis\n",
      "Mean Squared Error: 865395.3113869954\n",
      "R-squared: 0.1128396086327208\n",
      "Polynomial Regression (Degree = 2):\n",
      "Mean Squared Error: 761523.7120068332\n",
      "R-squared: 0.21932362529599247\n",
      "Random Forest Regression:\n",
      "Mean Squared Error: 614088.0958580723\n",
      "R-squared: 0.3704673132764311\n",
      "Neural Network Regression:\n",
      "Mean Squared Error: 882741.6089391984\n",
      "R-squared: 0.09505704392189351\n",
      "Gradient Boosting Regression:\n",
      "Mean Squared Error: 855716.59451248\n",
      "R-squared: 0.12276174957495256\n"
     ]
    }
   ],
   "source": [
    "# Predictor Variables: Radius, Semi-Major Axis, Planet Temperature, Eccentricity  \n",
    "# Response Variable: Mass\n",
    "\n",
    "multiple_linear_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_eqt', 'pl_orbeccen'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis', 'Planet Temperature', 'Eccentricity'], 'Planet Mass')\n",
    "polynomial_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_eqt', 'pl_orbeccen'], 'pl_bmasse', 2, ['Planet Radius', 'Semi-Major Axis', 'Planet Temperature', 'Eccentricity'], 'Planet Mass')\n",
    "random_forest_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_eqt', 'pl_orbeccen'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis', 'Planet Temperature', 'Eccentricity'], 'Planet Mass', n_estimators=100, show_feature_importance=False, log=False)  \n",
    "neural_network_regression(df, ['pl_rade', 'pl_orbsmax', 'pl_eqt', 'pl_orbeccen'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis', 'Planet Temperature', 'Eccentricity'], 'Planet Mass', log=False)\n",
    "gradient_boosting_regression_analysis(df, ['pl_rade', 'pl_orbsmax', 'pl_eqt', 'pl_orbeccen'], 'pl_bmasse', ['Planet Radius', 'Semi-Major Axis', 'Planet Temperature', 'Eccentricity'], 'Planet Mass', n_estimators=100, learning_rate=0.1, log=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
